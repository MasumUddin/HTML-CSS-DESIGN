{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasumUddin/HTML-CSS-DESIGN/blob/master/federated_reconstruction_for_matrix_factorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AftvNA5VMemJ"
      },
      "source": [
        "# Federated Reconstruction for Matrix Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxV9o4VmWNti"
      },
      "source": [
        "Main object of this project is to predict movie ratings given by some users by using Federated learning for matrix factorization by using stochastic gradient descent method optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8pu6-dckG_u",
        "outputId": "c04eb281-4b3f-484d-8312-31afaa3cbeda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.23.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated-nightly\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install numpy --upgrade\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2txfde-th95B",
        "outputId": "8dbf4633-81e4-4db0-961a-02b0b4672465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-62a7c070db7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_federated\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/pywrap_tf_session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_TF_SetTarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_TF_SetConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: SystemError: <built-in method __contains__ of dict object at 0x7fce9550cbe0> returned a result with an error set",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import collections\n",
        "import functools\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "229PrhyXaw_Y"
      },
      "source": [
        "## Background: Matrix Factorization\n",
        "\n",
        "[Matrix factorization](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)) has been a historically popular technique for learning recommendations and embedding representations for items based on user interactions. The goal of this project is to predict user ratings, where there are $n$ users and $m$ movies, and users have rated some movies. Given a user, we use their rating history and the ratings of similar users to predict the user's ratings for movies they haven't seen. If we have a model that can predict ratings, it's easy to recommend users new movies that they'll enjoy.\n",
        "\n",
        "For this task, it's useful to represent users' ratings as an $n \\times m$ matrix $R$:\n",
        "\n",
        "![Matrix Factorization Motivation (CC BY-SA 3.0; Wikipedia User Moshanin)](https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif)\n",
        "\n",
        "This matrix is generally sparse, since users typically only see a small fraction of the movies in the dataset. The output of matrix factorization is two matrices: an $n \\times k$ matrix $U$ representing $k$-dimensional user embeddings for each user, and an $m \\times k$ matrix $I$ representing $k$-dimensional item embeddings for each item. The simplest training objective is to ensure that the dot product of user and item embeddings are predictive of observed ratings $O$:\n",
        "\n",
        "$$argmin_{U,I}  \\sum_{(u, i) \\in O} (R_{ui} - U_u I_i^T)^2$$\n",
        "\n",
        "This is equivalent to minimizing the mean squared error between observed ratings and ratings predicted by taking the dot product of the corresponding user and item embeddings. Another way to interpret this is that this ensures that $R \\approx UI^T$ for known ratings, hence \"matrix factorization\". If this is confusing, don't worry–we won't need to know the details of matrix factorization for the rest of the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O37nOQRvAjw"
      },
      "source": [
        "## Exploring MovieLens Data\n",
        "\n",
        "Let's start by loading the [MovieLens 1M](https://grouplens.org/datasets/movielens/1m/) data, which consists of 1,000,209 movie ratings from 6040 users on 3706 movies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwxoBLaWneOE"
      },
      "outputs": [],
      "source": [
        "def download_movielens_data(dataset_path):\n",
        "  \"\"\"Downloads and copies MovieLens data to local /tmp directory.\"\"\"\n",
        "  if dataset_path.startswith('http'):\n",
        "    r = requests.get(dataset_path)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall(path='/tmp')\n",
        "  else:\n",
        "    tf.io.gfile.makedirs('/tmp/ml-1m/')\n",
        "    for filename in ['ratings.dat', 'movies.dat', 'users.dat']:\n",
        "      tf.io.gfile.copy(\n",
        "          os.path.join(dataset_path, filename),\n",
        "          os.path.join('/tmp/ml-1m/', filename),\n",
        "          overwrite=True)\n",
        "\n",
        "download_movielens_data('http://files.grouplens.org/datasets/movielens/ml-1m.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6_bskRUniqB"
      },
      "outputs": [],
      "source": [
        "def load_movielens_data(\n",
        "    data_directory: str = \"/tmp\",\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "  \"\"\"Loads pandas DataFrames for ratings, movies, users from data directory.\"\"\"\n",
        "  # Load pandas DataFrames from data directory. Assuming data is formatted as\n",
        "  # specified in http://files.grouplens.org/datasets/movielens/ml-1m-README.txt.\n",
        "  ratings_df = pd.read_csv(\n",
        "      os.path.join(data_directory, \"ml-1m\", \"ratings.dat\"),\n",
        "      sep=\"::\",\n",
        "      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], engine=\"python\")\n",
        "  movies_df = pd.read_csv(\n",
        "      os.path.join(data_directory, \"ml-1m\", \"movies.dat\"),\n",
        "      sep=\"::\",\n",
        "      names=[\"MovieID\", \"Title\", \"Genres\"], engine=\"python\")\n",
        "  \n",
        "  users_df = pd.read_csv(\n",
        "      os.path.join(data_directory, \"ml-1m\", \"users.dat\"),\n",
        "      sep=\"::\",\n",
        "      names=['UserID','Gender','Age','Occupation','Zip-code'], engine=\"python\")\n",
        "\n",
        "  # Create dictionaries mapping from old IDs to new (remapped) IDs for both\n",
        "  # MovieID and UserID. Use the movies and users present in ratings_df to\n",
        "  # determine the mapping, since movies and users without ratings are unneeded.\n",
        "  movie_mapping = {\n",
        "      old_movie: new_movie for new_movie, old_movie in enumerate(\n",
        "          ratings_df.MovieID.astype(\"category\").cat.categories)\n",
        "  }\n",
        "  user_mapping = {\n",
        "      old_user: new_user for new_user, old_user in enumerate(\n",
        "          ratings_df.UserID.astype(\"category\").cat.categories)\n",
        "  }\n",
        "\n",
        "  # Map each DataFrame consistently using the now-fixed mapping.\n",
        "  ratings_df.MovieID = ratings_df.MovieID.map(movie_mapping)\n",
        "  ratings_df.UserID = ratings_df.UserID.map(user_mapping)\n",
        "  movies_df.MovieID = movies_df.MovieID.map(movie_mapping)\n",
        "\n",
        "  # Remove nulls resulting from some movies being in movies_df but not\n",
        "  # ratings_df.\n",
        "  movies_df = movies_df[pd.notnull(movies_df.MovieID)]\n",
        "\n",
        "  return ratings_df, movies_df,users_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqVrh1o9t1cZ"
      },
      "source": [
        "Let's load and explore a couple Pandas DataFrames containing the rating and movie data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkAh5nt_n4ll"
      },
      "outputs": [],
      "source": [
        "ratings_df, movies_df,users_df = load_movielens_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aNtIwvNuP7v"
      },
      "source": [
        "We can see that each rating example has a rating from 1-5, a corresponding UserID, a corresponding MovieID, and a timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4qap4n-C83I"
      },
      "outputs": [],
      "source": [
        "ratings_df.head(n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5og9HO-ZubIJ"
      },
      "source": [
        "Each movie has a title and potentially multiple genres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TyN-30NC91Z"
      },
      "outputs": [],
      "source": [
        "movies_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWsip1k5ue5B"
      },
      "source": [
        "It's always a good idea to understand basic statistics of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7fNMAxdZ4DQ"
      },
      "outputs": [],
      "source": [
        "users_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I1jgmDOCqt4"
      },
      "outputs": [],
      "source": [
        "print('Num users:', len(set(ratings_df.UserID)))\n",
        "print('Num movies:', len(set(ratings_df.MovieID)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aO07Lg21Joa"
      },
      "outputs": [],
      "source": [
        "ratings = ratings_df.Rating.tolist()\n",
        "\n",
        "plt.hist(ratings, bins=5)\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Rating')\n",
        "plt.show()\n",
        "\n",
        "print('Average rating:', np.mean(ratings))\n",
        "print('Median rating:', np.median(ratings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poMbHDQguqPA"
      },
      "source": [
        "We can also plot the most popular movie genres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gYdfRoOw04z"
      },
      "outputs": [],
      "source": [
        "movie_genres_list = movies_df.Genres.tolist()\n",
        "# Count the number of times each genre describes a movie.\n",
        "genre_count = collections.defaultdict(int)\n",
        "for genres in movie_genres_list:\n",
        "  curr_genres_list = genres.split('|')\n",
        "  for genre in curr_genres_list:\n",
        "    genre_count[genre] += 1\n",
        "genre_name_list, genre_count_list = zip(*genre_count.items())\n",
        "\n",
        "plt.figure(figsize=(11, 11))\n",
        "plt.pie(genre_count_list, labels=genre_name_list)\n",
        "plt.title('MovieLens Movie Genres')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evWb8hg8vk-P"
      },
      "source": [
        "This data is naturally partitioned into ratings from different users, so we'd expect some heterogeneity in data between clients. Below we display the most commonly rated movie genres for different users. We can observe significant differences between users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfAeZ7f0GlSo"
      },
      "outputs": [],
      "source": [
        "def print_top_genres_for_user(ratings_df, movies_df, user_id):\n",
        "  \"\"\"Prints top movie genres for user with ID user_id.\"\"\"\n",
        "  user_ratings_df = ratings_df[ratings_df.UserID == user_id]\n",
        "  movie_ids = user_ratings_df.MovieID\n",
        "\n",
        "  genre_count = collections.Counter()\n",
        "  for movie_id in movie_ids:\n",
        "    genres_string = movies_df[movies_df.MovieID == movie_id].Genres.tolist()[0]\n",
        "    for genre in genres_string.split('|'):\n",
        "      genre_count[genre] += 1\n",
        "\n",
        "  print(f'\\nFor user {user_id}:')\n",
        "  for (genre, freq) in genre_count.most_common(5):\n",
        "    print(f'{genre} was rated {freq} times')\n",
        "\n",
        "print_top_genres_for_user(ratings_df, movies_df, user_id=0)\n",
        "print_top_genres_for_user(ratings_df, movies_df, user_id=10)\n",
        "print_top_genres_for_user(ratings_df, movies_df, user_id=19)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p88NsfPwTOP"
      },
      "source": [
        "## Preprocessing MovieLens Data\n",
        "\n",
        "We'll now prepare the MovieLens dataset as a list of `tf.data.Dataset`s representing each user's data for use with TFF.\n",
        "\n",
        "We implement two functions:\n",
        "* `create_tf_datasets`: takes our ratings DataFrame and produces a list of user-split `tf.data.Dataset`s.\n",
        "* `split_tf_datasets`: takes a list of datasets and splits them into train/val/test by *user*, so the val/test sets contain only ratings from users **unseen** during training. Typically in standard centralized matrix factorization we actually split so that the val/test sets contain held-out ratings from **seen** users, since unseen users don't have user embeddings. In our case, we'll see later that the approach we use to enable matrix factorization in FL also enables quickly reconstructing user embeddings for unseen users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHwb2AsvtIwO"
      },
      "outputs": [],
      "source": [
        "def create_tf_datasets(ratings_df: pd.DataFrame,\n",
        "                       batch_size: int = 1,\n",
        "                       max_examples_per_user: Optional[int] = None,\n",
        "                       max_clients: Optional[int] = None) -> List[tf.data.Dataset]:\n",
        "  \"\"\"Creates TF Datasets containing the movies and ratings for all users.\"\"\"\n",
        "  num_users = len(set(ratings_df.UserID))\n",
        "  # Optionally limit to `max_clients` to speed up data loading.\n",
        "  if max_clients is not None:\n",
        "    num_users = min(num_users, max_clients)\n",
        "\n",
        "  def rating_batch_map_fn(rating_batch):\n",
        "    \"\"\"Maps a rating batch to an OrderedDict with tensor values.\"\"\"\n",
        "    # Each example looks like: {x: movie_id, y: rating}.\n",
        "    # We won't need the UserID since each client will only look at their own\n",
        "    # data.\n",
        "    return collections.OrderedDict([\n",
        "        (\"x\", tf.cast(rating_batch[:, 1:2], tf.int64)),\n",
        "        (\"y\", tf.cast(rating_batch[:, 2:3], tf.float32))\n",
        "    ])\n",
        "\n",
        "  tf_datasets = []\n",
        "  for user_id in range(num_users):\n",
        "    # Get subset of ratings_df belonging to a particular user.\n",
        "    user_ratings_df = ratings_df[ratings_df.UserID == user_id]\n",
        "\n",
        "    tf_dataset = tf.data.Dataset.from_tensor_slices(user_ratings_df)\n",
        "\n",
        "    # Define preprocessing operations.\n",
        "    tf_dataset = tf_dataset.take(max_examples_per_user).shuffle(\n",
        "        buffer_size=max_examples_per_user, seed=42).batch(batch_size).map(\n",
        "        rating_batch_map_fn,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    tf_datasets.append(tf_dataset)\n",
        "\n",
        "  return tf_datasets\n",
        "\n",
        "\n",
        "def split_tf_datasets(\n",
        "    tf_datasets: List[tf.data.Dataset],\n",
        "    train_fraction: float = 0.75,\n",
        "    val_fraction: float = 0.05,\n",
        ") -> Tuple[List[tf.data.Dataset], List[tf.data.Dataset], List[tf.data.Dataset]]:\n",
        "  \"\"\"Splits a list of user TF datasets into train/val/test by user.\n",
        "  \"\"\"\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(tf_datasets)\n",
        "\n",
        "  train_idx = int(len(tf_datasets) * train_fraction)\n",
        "  val_idx = int(len(tf_datasets) * (train_fraction + val_fraction))\n",
        "\n",
        "  # Note that the val and test data contains completely different users, not\n",
        "  # just unseen ratings from train users.\n",
        "  return (tf_datasets[:train_idx], tf_datasets[train_idx:val_idx],\n",
        "          tf_datasets[val_idx:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6pJVpHfns9q"
      },
      "outputs": [],
      "source": [
        "# We limit the number of clients to speed up dataset creation. Feel free to pass\n",
        "# max_clients=None to load all clients' data.\n",
        "tf_datasets = create_tf_datasets(\n",
        "    ratings_df=ratings_df,\n",
        "    batch_size=5,\n",
        "    max_examples_per_user=100,\n",
        "    max_clients=500)\n",
        "\n",
        "# Split the ratings into training/val/test by client.\n",
        "tf_train_datasets, tf_val_datasets, tf_test_datasets = split_tf_datasets(\n",
        "    tf_datasets,\n",
        "    train_fraction=0.75,\n",
        "    val_fraction=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2SdGARZ0-cm"
      },
      "source": [
        "As a quick check, we can print a batch of training data. We can see that each individual example contains a MovieID under the \"x\" key and a rating under the \"y\" key. Note that we won't need the UserID since each user only sees their own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D2rCgcwFP4E"
      },
      "outputs": [],
      "source": [
        "print(next(iter(tf_train_datasets[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOaSLuFK18G7"
      },
      "source": [
        "We can plot a histogram showing the number of ratings per user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98VwSFBe1GPM"
      },
      "outputs": [],
      "source": [
        "def count_examples(curr_count, batch):\n",
        "  return curr_count + tf.size(batch['x'])\n",
        "\n",
        "num_examples_list = []\n",
        "# Compute number of examples for every other user.\n",
        "for i in range(0, len(tf_train_datasets), 2):\n",
        "  num_examples = tf_train_datasets[i].reduce(tf.constant(0), count_examples).numpy()\n",
        "  num_examples_list.append(num_examples)\n",
        "\n",
        "plt.hist(num_examples_list, bins=10)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Number of Examples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqz6oRm22FWM"
      },
      "source": [
        "Now that we've loaded and explored the data, we'll discuss how to bring matrix factorization to federated learning. Along the way, we'll motivate partially local federated learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMZLj5WprMJM"
      },
      "source": [
        "## Bringing Matrix Factorization to FL\n",
        "\n",
        "While matrix factorization has been traditionally used in centralized settings, it's especially relevant in federated learning:  user ratings may live on separate client devices, and we may want to learn embeddings and recommendations for users and items without centralizing the data. Since each user has a corresponding user embedding, it's natural to have each client store their user embedding–this scales much better than a central server storing all the user embeddings.\n",
        "\n",
        "One proposal for bringing matrix factorization to FL goes as follows:\n",
        "1. The server stores and sends the item matrix $I$ to sampled clients each round\n",
        "2. Clients update the item matrix and their personal user embedding $U_u$ using SGD on the above objective\n",
        "3. Updates to $I$ are aggregated on the server, updating the server copy of $I$ for the next round\n",
        "\n",
        "This approach is *partially local*–that is, some client parameters are never aggregated by the server. Though this approach is appealing, it requires clients to maintain state across rounds, namely their user embeddings. Stateful federated algorithms are less appropriate for cross-device FL settings: in these settings the population size is often much larger than the number of clients that participate in each round, and a client usually participates at most once during the training process. Besides relying on state that may not be initialized, stateful algorithms can result in performance degradation in cross-device settings due to state getting *stale* when clients are infrequently sampled. Importantly, in the matrix factorization setting, a stateful algorithm leads to all unseen clients missing trained user embeddings, and in large-scale training the majority of users may be unseen. For more on the motivation for stateless algorithms in cross-device FL, see [Wang et al. 2021 Sec. 3.1.1](https://arxiv.org/pdf/2107.06917.pdf) and [Reddi et al. 2020 Sec. 5.1](https://arxiv.org/abs/2003.00295).\n",
        "\n",
        "Federated Reconstruction ([Singhal et al. 2021](https://arxiv.org/abs/2102.03448)) is a stateless alternative to the aforementioned approach. The key idea is that instead of storing user embeddings across rounds, clients reconstruct user embeddings when needed. When FedRecon is applied to matrix factorization, training proceeds as follows:\n",
        "1. The server stores and sends the item matrix $I$ to sampled clients each round\n",
        "2. Each client freezes $I$ and trains their user embedding $U_u$ using one or more steps of SGD (reconstruction)\n",
        "3. Each client freezes $U_u$ and trains $I$ using one or more steps of SGD\n",
        "4. Updates to $I$ are aggregated across users, updating the server copy of $I$ for the next round\n",
        "\n",
        "This approach does not require clients to maintain state across rounds. The authors also show in the paper that this method leads to fast reconstruction of user embeddings for unseen clients (Sec. 4.2, Fig. 3, and Table 1), allowing the majority of clients who do not participate in training to have a trained model, enabling recommendations for these clients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imwLf1zksCjN"
      },
      "source": [
        "## Defining the Model\n",
        "\n",
        "We'll next define the local matrix factorization model to be trained on client devices. This model will include the full item matrix $I$ and a single user embedding $U_u$ for client $u$. Note that clients will not need to store the full user matrix $U$.\n",
        "\n",
        "We'll define the following:\n",
        "- `UserEmbedding`: a simple Keras layer representing a single `num_latent_factors`-dimensional user embedding.\n",
        "- `get_matrix_factorization_model`: a function that returns a [`tff.learning.reconstruction.Model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/reconstruction/Model) containing the model logic, including which layers are globally aggregated on the server and which layers remain local. We need this additional information to initialize the Federated Reconstruction training process. Here we produce the `tff.learning.reconstruction.Model` from a Keras model using [`tff.learning.reconstruction.from_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/reconstruction/from_keras_model). Similar to `tff.learning.Model`, we can also implement a custom `tff.learning.reconstruction.Model` by implementing the class interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSLMxPDP3D72"
      },
      "outputs": [],
      "source": [
        "class UserEmbedding(tf.keras.layers.Layer):\n",
        "  \"\"\"Keras layer representing an embedding for a single user, used below.\"\"\"\n",
        "\n",
        "  def __init__(self, num_latent_factors, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_latent_factors = num_latent_factors\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = self.add_weight(\n",
        "        shape=(1, self.num_latent_factors),\n",
        "        initializer='uniform',\n",
        "        dtype=tf.float32,\n",
        "        name='UserEmbeddingKernel')\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.embedding\n",
        "\n",
        "  def compute_output_shape(self):\n",
        "    return (1, self.num_latent_factors)\n",
        "\n",
        "\n",
        "def get_matrix_factorization_model(\n",
        "    num_items: int,\n",
        "    num_latent_factors: int) -> tff.learning.reconstruction.Model:\n",
        "  \"\"\"Defines a Keras matrix factorization model.\"\"\"\n",
        "  # Layers with variables will be partitioned into global and local layers.\n",
        "  # We'll pass this to `tff.learning.reconstruction.from_keras_model`.\n",
        "  global_layers = []\n",
        "  local_layers = []\n",
        "\n",
        "  # Extract the item embedding.\n",
        "  item_input = tf.keras.layers.Input(shape=[1], name='Item')\n",
        "  item_embedding_layer = tf.keras.layers.Embedding(\n",
        "      num_items,\n",
        "      num_latent_factors,\n",
        "      name='ItemEmbedding')\n",
        "  global_layers.append(item_embedding_layer)\n",
        "  flat_item_vec = tf.keras.layers.Flatten(name='FlattenItems')(\n",
        "      item_embedding_layer(item_input))\n",
        "\n",
        "  # Extract the user embedding.\n",
        "  user_embedding_layer = UserEmbedding(\n",
        "      num_latent_factors,\n",
        "      name='UserEmbedding')\n",
        "  local_layers.append(user_embedding_layer)\n",
        "\n",
        "  # The item_input never gets used by the user embedding layer,\n",
        "  # but this allows the model to directly use the user embedding.\n",
        "  flat_user_vec = user_embedding_layer(item_input)\n",
        "\n",
        "  # Compute the dot product between the user embedding, and the item one.\n",
        "  pred = tf.keras.layers.Dot(\n",
        "      1, normalize=False, name='Dot')([flat_user_vec, flat_item_vec])\n",
        "\n",
        "  input_spec = collections.OrderedDict(\n",
        "      x=tf.TensorSpec(shape=[None, 1], dtype=tf.int64),\n",
        "      y=tf.TensorSpec(shape=[None, 1], dtype=tf.float32))\n",
        "\n",
        "  model = tf.keras.Model(inputs=item_input, outputs=pred)\n",
        "\n",
        "  return tff.learning.reconstruction.from_keras_model(\n",
        "      keras_model=model,\n",
        "      global_layers=global_layers,\n",
        "      local_layers=local_layers,\n",
        "      input_spec=input_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B3FPaRiwY3n"
      },
      "source": [
        "Analagous to the interface for Federated Averaging, the interface for Federated Reconstruction expects a `model_fn` with no arguments that returns a `tff.learning.reconstruction.Model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNBRQW9EwneZ"
      },
      "outputs": [],
      "source": [
        "# This will be used to produce our training process.\n",
        "# User and item embeddings will be 50-dimensional.\n",
        "model_fn = functools.partial(\n",
        "    get_matrix_factorization_model,\n",
        "    num_items=3706,\n",
        "    num_latent_factors=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQVpVIfnwvPg"
      },
      "source": [
        "We'll next define `loss_fn` and `metrics_fn`, where `loss_fn` is a no-argument function returning a Keras loss to use to train the model, and `metrics_fn` is a no-argument function returning a list of Keras metrics for evaluation. These are needed to build the training and evaluation computations.\n",
        "\n",
        "We'll use Mean Squared Error as the loss, as mentioned above. For evaluation we'll use rating accuracy (when the model's predicted dot product is rounded to the nearest whole number, how often does it match the label rating?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDJUfeSNwxIL"
      },
      "outputs": [],
      "source": [
        "class RatingAccuracy(tf.keras.metrics.Mean):\n",
        "  \"\"\"Keras metric computing accuracy of reconstructed ratings.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               name: str = 'rating_accuracy',\n",
        "               **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "\n",
        "  def update_state(self,\n",
        "                   y_true: tf.Tensor,\n",
        "                   y_pred: tf.Tensor,\n",
        "                   sample_weight: Optional[tf.Tensor] = None):\n",
        "    absolute_diffs = tf.abs(y_true - y_pred)\n",
        "    # A [batch_size, 1] tf.bool tensor indicating correctness within the\n",
        "    # threshold for each example in a batch. A 0.5 threshold corresponds\n",
        "    # to correctness when predictions are rounded to the nearest whole\n",
        "    # number.\n",
        "    example_accuracies = tf.less_equal(absolute_diffs, 0.8) \n",
        "    super().update_state(example_accuracies, sample_weight=sample_weight)\n",
        "\n",
        "\n",
        "loss_fn = lambda: tf.keras.losses.MeanSquaredError()\n",
        "metrics_fn = lambda: [RatingAccuracy()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecM_vru8xg2j"
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "Now we have everything we need to define the training process. One important difference from the [interface for Federated Averaging](https://www.tensorflow.org/federated/api_docs/python/tff/learning/build_federated_averaging_process) is that we now pass in a `reconstruction_optimizer_fn`, which will be used when reconstructing local parameters (in our case, user embeddings). It's generally reasonable to use `SGD` here, with a similar or slightly lower learning rate than the client optimizer learning rate. We provide a working configuration below. This hasn't been carefully tuned, so feel free to play around with different values.\n",
        "\n",
        "Check out the [documentation](https://www.tensorflow.org/federated/api_docs/python/tff/learning/reconstruction/build_training_process) for more details and options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQsX0FgtwsoE"
      },
      "outputs": [],
      "source": [
        "# We'll use this by doing:\n",
        "# state = training_process.initialize()\n",
        "# state, metrics = training_process.next(state, federated_train_data)\n",
        "\n",
        "training_process = tff.learning.reconstruction.build_training_process(\n",
        "    model_fn=model_fn,\n",
        "    loss_fn=loss_fn,\n",
        "    metrics_fn=metrics_fn,\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0),\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0),\n",
        "    reconstruction_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1))\n",
        "\n",
        "    # server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.001,\n",
        "    # beta_1=0.9,\n",
        "    # beta_2=0.999,\n",
        "    # epsilon=1e-01,\n",
        "    # amsgrad=False),\n",
        "    # client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.001,\n",
        "    # beta_1=0.9,\n",
        "    # beta_2=0.999,\n",
        "    # epsilon=1e-01,\n",
        "    # amsgrad=False),\n",
        "    # reconstruction_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.001,\n",
        "    # beta_1=0.9,\n",
        "    # beta_2=0.999,\n",
        "    # epsilon=1e-01,\n",
        "    # amsgrad=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssbbds4vzhXJ"
      },
      "source": [
        "We can also define a computation for evaluating our trained global model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHi7J330PtxO"
      },
      "outputs": [],
      "source": [
        "# We'll use this by doing:\n",
        "# eval_metrics = evaluation_computation(state.model, tf_val_datasets)\n",
        "# where `state` is the state from the training process above.\n",
        "evaluation_computation = tff.learning.reconstruction.build_federated_evaluation(\n",
        "    model_fn,\n",
        "    loss_fn=loss_fn,\n",
        "    metrics_fn=metrics_fn,\n",
        "    reconstruction_optimizer_fn=functools.partial(\n",
        "            tf.keras.optimizers.SGD,learning_rate=0.001,momentum=.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_V_ZwlE0DSl"
      },
      "source": [
        "We can initialize the training process state and examine it. Most importantly, we can see that this server state only stores item variables (currently randomly initialized) and not any user embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_kOjFVKQoNX"
      },
      "outputs": [],
      "source": [
        "state = training_process.initialize()\n",
        "print(state.model)\n",
        "print('Item variables shape:', state.model.trainable[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPFqgTV21lJO"
      },
      "source": [
        "We can also try to evaluate our randomly initialized model on validation clients. Federated Reconstruction evaluation here involves the following:\n",
        "\n",
        "1. The server sends the item matrix $I$ to sampled evaluation clients\n",
        "2. Each client freezes $I$ and trains their user embedding $U_u$ using one or more steps of SGD (reconstruction)\n",
        "3. Each client calculates loss and metrics using the server $I$ and reconstructed $U_u$ on an unseen portion of their local data\n",
        "4. Losses and metrics are averaged across users to calculate overall loss and metrics\n",
        "\n",
        "Note that steps 1 and 2 are the same as for training. This connection is important, since training the same way we evaluate leads to a form of *meta-learning*, or learning how to learn. In this case, the model is learning how to learn global variables (item matrix) that lead to performant reconstruction of local variables (user embeddings). For more on this, see [Sec. 4.2](https://arxiv.org/abs/2102.03448) of the paper.\n",
        "\n",
        "It's also important for steps 2 and 3 to be performed using disjoint portions of clients' local data, to ensure fair evaluation. By default, both the training process and evaluation computation use every other example for reconstruction and use the other half post-reconstruction. This behavior can be customized using the `dataset_split_fn` argument (we'll explore this further later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiBOGFsWWBiU"
      },
      "outputs": [],
      "source": [
        "# We shouldn't expect good evaluation results here, since we haven't trained\n",
        "# yet!\n",
        "eval_metrics = evaluation_computation(state.model, tf_val_datasets)\n",
        "print('Initial Eval:', eval_metrics['eval'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZUZwjWp4iJu"
      },
      "source": [
        "We can next try running a round of training. To make things more realistic, we'll sample 50 clients per round randomly without replacement. We should still expect train metrics to be poor, since we're only doing one round of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOTfqrVcVfJf"
      },
      "outputs": [],
      "source": [
        "federated_train_data = np.random.choice(tf_train_datasets, size=50, replace=False).tolist()\n",
        "state, metrics = training_process.next(state, federated_train_data)\n",
        "print(f'Train metrics:', metrics['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr3ZS9jz5Mj0"
      },
      "source": [
        "Now let's set up a training loop to train over multiple rounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJBzOPNYwp9q"
      },
      "outputs": [],
      "source": [
        "NUM_ROUNDS =50\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "state = training_process.initialize()\n",
        "max_accuracy=-1\n",
        "# This may take a couple minutes to run.\n",
        "for i in range(NUM_ROUNDS):\n",
        "  federated_train_data = np.random.choice(tf_train_datasets, size=25, replace=False).tolist()\n",
        "  state, metrics = training_process.next(state, federated_train_data)\n",
        "  print(f'Train round {i}:','loss: ', metrics['train']['loss'],' - accuracy: ', metrics['train']['rating_accuracy']  )\n",
        "  train_losses.append(metrics['train']['loss'])\n",
        "  train_accs.append(metrics['train']['rating_accuracy'])\n",
        "  max_accuracy=max(max_accuracy,metrics['train']['rating_accuracy'])\n",
        "\n",
        "eval_metrics = evaluation_computation(state.model, tf_val_datasets)\n",
        "print('Final Eval:', eval_metrics['eval'])\n",
        "print('maximum accuracy:',max_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM-jAGNm5di7"
      },
      "source": [
        "We can plot training loss and accuracy over rounds. The hyperparameters in this notebook have not been carefully tuned, so feel free to try different clients per round, learning rates, number of rounds, and total number of clients to improve these results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6w702JmR-3V"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(NUM_ROUNDS), train_losses)\n",
        "plt.ylabel('Train Loss')\n",
        "plt.xlabel('Round')\n",
        "plt.title('Train Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(NUM_ROUNDS), train_accs)\n",
        "plt.ylabel('Train Accuracy')\n",
        "plt.xlabel('Round')\n",
        "plt.title('Train Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTzKkT-a5kgX"
      },
      "source": [
        "Finally, we can calculate metrics on an unseen test set when we're finished tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq0UxEBBJcR-"
      },
      "outputs": [],
      "source": [
        "steps=5\n",
        "max_fin=-1\n",
        "for x in range(1,steps):\n",
        "  eval_metrics = evaluation_computation(state.model, tf_test_datasets)\n",
        "  print('Final Test:', eval_metrics['eval'])\n",
        "  max_fin=max(max_fin,eval_metrics['eval']['rating_accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr2fRxic6Lfi"
      },
      "source": [
        "## Further Explorations\n",
        "\n",
        "Nice work on completing this notebook. We suggest the following exercises to explore partially local federated learning further, roughly ordered by increasing difficulty:\n",
        "\n",
        "* Typical implementations of Federated Averaging take multiple local passes (epochs) over the data (in addition to taking one pass over the data across multiple batches). For Federated Reconstruction we may want to control the number of steps separately for reconstruction and post-reconstruction training. Passing the `dataset_split_fn` argument to the training and evaluation computation builders enables control of the number of steps and epochs over both reconstruction and post-reconstruction datasets. As an exercise, try performing 3 local epochs of reconstruction training, capped at 50 steps and 1 local epoch of post-reconstruction training, capped at 50 steps. Hint: you'll find [`tff.learning.reconstruction.build_dataset_split_fn`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/reconstruction/build_dataset_split_fn) helpful. Once you've done this, try tuning these hyperparameters and other related ones like learning rates and batch size to get better results.\n",
        "\n",
        "* The default behavior of Federated Reconstruction training and evaluation is to split clients' local data in half for each of reconstruction and post-reconstruction. In cases where clients have very little local data, it can be reasonable to reuse data for reconstruction and post-reconstruction for the training process only (not for evaluation, this will lead to unfair evaluation). Try making this change for the training process, ensuring the `dataset_split_fn` for evaluation still keeps reconstruction and post-reconstruction data disjoint. Hint: [`tff.learning.reconstruction.simple_dataset_split_fn`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/reconstruction/simple_dataset_split_fn) might be useful.\n",
        "\n",
        "* Above, we produced a `tff.learning.Model` from a Keras model using `tff.learning.reconstruction.from_keras_model`. We can also implement a custom model using pure TensorFlow 2.0 by [implementing the model interface](https://www.tensorflow.org/federated/api_docs/python/tff/learning/reconstruction/Model). Try modifying `get_matrix_factorization_model` to build and return a class that extends `tff.learning.reconstruction.Model`, implementing its methods. Hint: the source code of [`tff.learning.reconstruction.from_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/reconstruction/from_keras_model) provides an example of extending the `tff.learning.reconstruction.Model` class. Refer also to the [custom model implementation in the EMNIST image classification tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification#customizing_the_model_implementation) for a similar exercise in extending a `tff.learning.Model`.\n",
        "\n",
        "* In this tutorial, we've motivated partially local federated learning in the context of matrix factorization, where sending user embeddings to the server would trivially leak user preferences. We can also apply Federated Reconstruction in other settings as a way to train more personal models (since part of the model is completely local to each user) while reducing communication (since local parameters are not sent to the server). In general, using the interface presented here we can take any federated model that would typically be trained fully globally and instead partition its variables into global variables and local variables. The example explored in the [Federated Reconstruction paper](https://arxiv.org/abs/2102.03448) is personal next word prediction: here, each user has their own local set of word embeddings for out-of-vocabulary words, enabling the model to capture users' slang and achieve personalization without additional communication. As an exercise, try implementing (as either a Keras model or a custom TensorFlow 2.0 model) a different model for use with Federated Reconstruction. A suggestion: implement an EMNIST classification model with a personal user embedding, where the personal user embedding is concatenated to the CNN image features before the last Dense layer of the model. You can reuse much of the code from this tutorial (e.g. the `UserEmbedding` class) and the [image classification tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification).\n",
        "\n",
        "\\\n",
        "If you're still looking for more on partially local federated learning, check out the [Federated Reconstruction paper](https://arxiv.org/abs/2102.03448) and [open-source experiment code](https://github.com/google-research/federated/tree/master/reconstruction)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "federated_reconstruction_for_matrix_factorization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}